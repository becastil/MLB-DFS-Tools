"""Utility API for powering the interactive simulation dashboard.

The goal of this module is to surface the latest data generated by the
existing optimizers/simulators so that the React dashboard can visualise it
without relying on mocked values.  The API aggregates data from the CSV files
that the tooling already produces (projections, optimizer exports and GPP
simulation summaries) and exposes a small JSON schema that the front-end can
consume.

Run with::

    uvicorn src.dashboard_api:app --reload

"""

from __future__ import annotations

import logging
import re
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import pandas as pd
from fastapi import FastAPI, HTTPException, Response
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from fastapi.staticfiles import StaticFiles
import sys
sys.path.append('.')  # Allow importing from src directory

# Firecrawl integration
from pipeline.data_sources.firecrawl_client import get_firecrawl_client
from pipeline.data_sources.mlb_extraction_schemas import MLBExtractionSchemas, MLBExtractionPrompts

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SPAStaticFiles(StaticFiles):
    """Custom StaticFiles handler that serves index.html for unmatched routes (SPA support)."""
    
    async def get_response(self, path: str, scope):
        try:
            return await super().get_response(path, scope)
        except Exception as ex:
            # If we get a 404, serve index.html instead to support client-side routing
            if getattr(ex, 'status_code', None) == 404:
                return await super().get_response("index.html", scope)
            raise ex


BASE_DIR = Path(__file__).resolve().parent.parent
OUTPUT_DIR = BASE_DIR / "output"
FRONTEND_DIST_DIR = BASE_DIR / "frontend" / "dist"


SITE_DIRECTORIES: Dict[str, Path] = {
    "dk": BASE_DIR / "dk_data",
    "fd": BASE_DIR / "fd_data",
    "ikb": BASE_DIR / "ikb_data",
}


@dataclass
class FileInfo:
    path: Path
    modified: datetime
    extra: Dict[str, Optional[int]]


def available_sites() -> List[str]:
    """Return the list of sites that have any supporting data files."""

    sites: List[str] = []
    for site, folder in SITE_DIRECTORIES.items():
        if folder.exists() and any(folder.glob("*.csv")):
            sites.append(site)
            continue

        # fall back to checking the output directory for artefacts
        pattern = f"{site}_*"
        if OUTPUT_DIR.exists() and list(OUTPUT_DIR.glob(pattern)):
            sites.append(site)

    return sorted(set(sites))


def _parse_float(value) -> Optional[float]:
    """Coerce strings such as "12.5%" or "$15" into floats."""

    if value is None:
        return None

    if isinstance(value, (int, float)):
        return float(value)

    stripped = str(value).strip()
    if not stripped or stripped in {"-", "NA", "nan", "None"}:
        return None

    cleaned = (
        stripped.replace("%", "")
        .replace("$", "")
        .replace(",", "")
        .replace("+", "")
    )

    try:
        return float(cleaned)
    except ValueError:
        return None


def _load_csv(path: Path) -> Optional[pd.DataFrame]:
    if not path.exists():
        return None

    try:
        return pd.read_csv(path)
    except pd.errors.EmptyDataError:
        return pd.DataFrame()


def _latest_file(pattern: str) -> Optional[FileInfo]:
    if not OUTPUT_DIR.exists():
        return None

    matches = sorted(OUTPUT_DIR.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)
    if not matches:
        return None

    latest = matches[0]
    return FileInfo(
        path=latest,
        modified=datetime.fromtimestamp(latest.stat().st_mtime),
        extra={},
    )


def _load_projections(site: str) -> Tuple[Optional[pd.DataFrame], Optional[FileInfo]]:
    projections_path = SITE_DIRECTORIES.get(site, BASE_DIR / f"{site}_data") / "projections.csv"
    df = _load_csv(projections_path)
    if df is None:
        return None, None

    file_info = FileInfo(
        path=projections_path,
        modified=datetime.fromtimestamp(projections_path.stat().st_mtime),
        extra={},
    )

    df.columns = [column.lower() for column in df.columns]
    return df, file_info


def _load_optimizer_export(site: str) -> Tuple[Optional[pd.DataFrame], Optional[FileInfo]]:
    file_info = _latest_file(f"{site}_optimal_lineups_*.csv")
    if not file_info:
        return None, None

    df = _load_csv(file_info.path)
    return df, file_info


def _load_simulation_lineups(site: str) -> Tuple[Optional[pd.DataFrame], Optional[FileInfo]]:
    file_info = _latest_file(f"{site}_gpp_sim_lineups_*.csv")
    if not file_info:
        return None, None

    match = re.search(r"_(\d+)_(\d+)\.csv$", file_info.path.name)
    if match:
        field_size, iterations = match.groups()
        file_info.extra.update(
            {"field_size": int(field_size), "iterations": int(iterations)}
        )

    df = _load_csv(file_info.path)
    return df, file_info


def _load_simulation_exposure(site: str) -> Tuple[Optional[pd.DataFrame], Optional[FileInfo]]:
    file_info = _latest_file(f"{site}_gpp_sim_player_exposure_*.csv")
    if not file_info:
        return None, None

    df = _load_csv(file_info.path)
    return df, file_info


def _format_timestamp(info: Optional[FileInfo]) -> Optional[str]:
    if not info:
        return None
    return info.modified.strftime("%Y-%m-%d %H:%M:%S")


def _build_metrics(
    projections_df: Optional[pd.DataFrame],
    optimizer_df: Optional[pd.DataFrame],
    simulation_info: Optional[FileInfo],
    sim_lineups: Optional[pd.DataFrame],
) -> List[Dict[str, str]]:
    metrics: List[Dict[str, str]] = []

    if projections_df is not None and not projections_df.empty:
        metrics.append(
            {
                "label": "Total Projections",
                "value": f"{len(projections_df):,}",
                "change": None,
            }
        )

    if simulation_info and simulation_info.extra.get("iterations"):
        iterations = simulation_info.extra["iterations"]
        metrics.append(
            {
                "label": "Sim Iterations",
                "value": f"{iterations:,}",
                "change": None,
            }
        )

    if optimizer_df is not None and not optimizer_df.empty:
        metrics.append(
            {
                "label": "Optimized Lineups",
                "value": f"{len(optimizer_df):,}",
                "change": None,
            }
        )

    if sim_lineups is not None and not sim_lineups.empty:
        win_column = next((col for col in sim_lineups.columns if col.lower().startswith("win")), None)
        if win_column:
            win_average = (
                sim_lineups[win_column]
                .apply(_parse_float)
                .dropna()
                .mean()
            )
            if win_average is not None and not pd.isna(win_average):
                metrics.append(
                    {
                        "label": "Avg. Win %",
                        "value": f"{win_average:.2f}%",
                        "change": None,
                    }
                )

    return metrics


def _build_quick_stats(
    projections_df: Optional[pd.DataFrame],
    optimizer_df: Optional[pd.DataFrame],
    sim_lineups: Optional[pd.DataFrame],
) -> List[Dict[str, str]]:
    stats: List[Dict[str, str]] = []

    if sim_lineups is not None and not sim_lineups.empty:
        stats.append(
            {
                "label": "Lineups Simulated",
                "value": f"{len(sim_lineups):,}",
            }
        )

    if optimizer_df is not None and not optimizer_df.empty:
        stats.append(
            {
                "label": "Optimizer Lineups",
                "value": f"{len(optimizer_df):,}",
            }
        )

    if projections_df is not None and "fpts" in projections_df.columns:
        stats.append(
            {
                "label": "Avg. Projection",
                "value": f"{projections_df['fpts'].mean():.2f}",
            }
        )

    if projections_df is not None and "own%" in projections_df.columns:
        stats.append(
            {
                "label": "Avg. Ownership",
                "value": f"{projections_df['own%'].mean():.2f}%",
            }
        )

    return stats


def _performance_chart(sim_lineups: Optional[pd.DataFrame]) -> List[Dict[str, float]]:
    if sim_lineups is None or sim_lineups.empty:
        return []

    metric_cols = {col.lower(): col for col in sim_lineups.columns}

    projection_col = metric_cols.get("fpts proj".lower())
    ceiling_col = metric_cols.get("ceiling")
    win_col = metric_cols.get("win %".lower())

    sortable = sim_lineups.copy()
    if win_col:
        sortable[win_col] = sortable[win_col].apply(_parse_float)
        sortable = sortable.sort_values(win_col, ascending=False)

    records: List[Dict[str, float]] = []
    for idx, row in sortable.head(12).iterrows():
        records.append(
            {
                "label": f"#{idx + 1}",
                "projection": float(row.get(projection_col, 0) or 0),
                "ceiling": float(row.get(ceiling_col, 0) or 0),
                "winRate": float(_parse_float(row.get(win_col, 0)) or 0),
            }
        )

    return records


def _ownership_chart(exposure_df: Optional[pd.DataFrame]) -> List[Dict[str, float]]:
    if exposure_df is None or exposure_df.empty:
        return []

    df = exposure_df.copy()
    df.columns = [column.strip().lower() for column in df.columns]

    def _col(name: str) -> Optional[str]:
        return next((col for col in df.columns if col.replace(".", "") == name), None)

    player_col = _col("player") or "player"
    sim_col = _col("sim own%")
    proj_col = _col("proj own%")

    if not sim_col or not proj_col:
        return []

    df["simulated"] = df[sim_col].apply(_parse_float)
    df["projected"] = df[proj_col].apply(_parse_float)
    df["leverage"] = df["simulated"] - df["projected"]

    df = df.sort_values("leverage", ascending=False).head(10)

    records: List[Dict[str, float]] = []
    for _, row in df.iterrows():
        records.append(
            {
                "player": row.get(player_col, ""),
                "projected": float(row.get("projected") or 0),
                "simulated": float(row.get("simulated") or 0),
                "leverage": float(row.get("leverage") or 0),
            }
        )

    return records


def _composition_chart(sim_lineups: Optional[pd.DataFrame]) -> List[Dict[str, float]]:
    if sim_lineups is None or sim_lineups.empty:
        return []

    stack_columns = [col for col in sim_lineups.columns if col.lower().startswith("stack")]
    if not stack_columns:
        return []

    counts: Dict[str, int] = {}
    for column in stack_columns:
        for value in sim_lineups[column].dropna():
            name = str(value)
            counts[name] = counts.get(name, 0) + 1

    total = sum(counts.values()) or 1
    return [
        {
            "name": name,
            "value": round(count / total * 100, 2),
        }
        for name, count in sorted(counts.items(), key=lambda x: x[1], reverse=True)[:8]
    ]


def _radar_chart(
    sim_lineups: Optional[pd.DataFrame],
    exposure_df: Optional[pd.DataFrame],
) -> List[Dict[str, float]]:
    if sim_lineups is None or sim_lineups.empty:
        return []

    metric_cols = {col.lower(): col for col in sim_lineups.columns}
    win_col = metric_cols.get("win %".lower())
    top_col = metric_cols.get("top 10%".lower())
    roi_col = metric_cols.get("roi%".lower())
    proj_col = metric_cols.get("fpts proj".lower())

    def _mean(column: Optional[str]) -> float:
        if not column:
            return 0.0
        values = sim_lineups[column].apply(_parse_float).dropna()
        if values.empty:
            return 0.0
        return float(values.mean())

    avg_projection = _mean(proj_col)
    max_projection = _parse_float(sim_lineups[proj_col].max()) if proj_col else None
    projection_score = 0.0
    if avg_projection and max_projection:
        projection_score = min(100.0, (avg_projection / max_projection) * 100.0)

    uniqueness_score = 0.0
    if exposure_df is not None and not exposure_df.empty:
        exposure = exposure_df.copy()
        exposure.columns = [column.strip().lower() for column in exposure.columns]
        proj_own_col = next(
            (col for col in exposure.columns if col.replace(".", "") == "proj own%"),
            None,
        )
        if proj_own_col:
            ownership = exposure[proj_own_col].apply(_parse_float).dropna()
            if not ownership.empty:
                uniqueness_score = max(0.0, 100.0 - float(ownership.mean()))

    radar = [
        {"stat": "Win Rate", "value": round(_mean(win_col), 2)},
        {"stat": "Top Finish", "value": round(_mean(top_col), 2)},
        {"stat": "ROI", "value": round(_mean(roi_col), 2)},
        {"stat": "Projection", "value": round(projection_score, 2)},
        {"stat": "Uniqueness", "value": round(uniqueness_score, 2)},
    ]

    return radar


def _contest_table(
    site: str,
    sim_lineups: Optional[pd.DataFrame],
    sim_info: Optional[FileInfo],
) -> List[Dict[str, Optional[float]]]:
    if sim_lineups is None or sim_lineups.empty:
        return []

    metric_cols = {col.lower(): col for col in sim_lineups.columns}
    projection_col = metric_cols.get("fpts proj".lower())
    roi_col = metric_cols.get("roi%".lower())
    avg_return_col = metric_cols.get("avg. return".lower())

    top_projection = None
    avg_projection = None
    avg_roi = None
    avg_return = None

    if projection_col:
        projections = sim_lineups[projection_col].apply(_parse_float).dropna()
        if not projections.empty:
            top_projection = float(projections.max())
            avg_projection = float(projections.mean())

    if roi_col:
        roi_values = sim_lineups[roi_col].apply(_parse_float).dropna()
        if not roi_values.empty:
            avg_roi = float(roi_values.mean())

    if avg_return_col:
        returns = sim_lineups[avg_return_col].apply(_parse_float).dropna()
        if not returns.empty:
            avg_return = float(returns.mean())

    field_size = sim_info.extra.get("field_size") if sim_info else None

    return [
        {
            "name": f"{site.upper()} Simulation",
            "entries": field_size,
            "topScore": top_projection,
            "avgScore": avg_projection,
            "avgReturn": avg_return,
            "roi": avg_roi,
        }
    ]


def _lineup_preview(sim_lineups: Optional[pd.DataFrame]) -> List[Dict[str, object]]:
    if sim_lineups is None or sim_lineups.empty:
        return []

    columns = list(sim_lineups.columns)
    metric_index = next(
        (idx for idx, col in enumerate(columns) if col.lower().startswith("fpts")),
        len(columns),
    )
    roster_columns = columns[:metric_index]

    metric_cols = {col.lower(): col for col in columns[metric_index:]}
    projection_col = metric_cols.get("fpts proj".lower())
    win_col = metric_cols.get("win %".lower())
    roi_col = metric_cols.get("roi%".lower())

    preview: List[Dict[str, object]] = []
    sortable = sim_lineups.copy()
    if win_col:
        sortable[win_col] = sortable[win_col].apply(_parse_float)
        sortable = sortable.sort_values(win_col, ascending=False)

    for _, row in sortable.head(8).iterrows():
        lineup = [str(row[col]) for col in roster_columns]
        preview.append(
            {
                "lineup": lineup,
                "projection": float(row.get(projection_col, 0) or 0),
                "winPct": float(_parse_float(row.get(win_col, 0)) or 0),
                "roi": float(_parse_float(row.get(roi_col, 0)) or 0),
            }
        )

    return preview


def _exposure_table(exposure_df: Optional[pd.DataFrame]) -> List[Dict[str, float]]:
    if exposure_df is None or exposure_df.empty:
        return []

    df = exposure_df.copy()
    df.columns = [column.strip().lower() for column in df.columns]

    player_col = next((col for col in df.columns if col.replace(".", "") == "player"), None)
    win_col = next((col for col in df.columns if col.startswith("win")), None)
    top_col = next((col for col in df.columns if col.startswith("top1")), None)
    cash_col = next((col for col in df.columns if col.startswith("cash")), None)
    sim_col = next((col for col in df.columns if col.startswith("sim own")), None)
    proj_col = next((col for col in df.columns if col.startswith("proj own")), None)

    records: List[Dict[str, float]] = []
    for _, row in df.head(20).iterrows():
        records.append(
            {
                "player": row.get(player_col, ""),
                "winPct": float(_parse_float(row.get(win_col, 0)) or 0),
                "topPct": float(_parse_float(row.get(top_col, 0)) or 0),
                "cashPct": float(_parse_float(row.get(cash_col, 0)) or 0),
                "simOwn": float(_parse_float(row.get(sim_col, 0)) or 0),
                "projOwn": float(_parse_float(row.get(proj_col, 0)) or 0),
            }
        )

    return records


def build_dashboard_payload(site: str) -> Dict[str, object]:
    projections_df, projections_info = _load_projections(site)
    optimizer_df, optimizer_info = _load_optimizer_export(site)
    sim_lineups, sim_info = _load_simulation_lineups(site)
    exposure_df, exposure_info = _load_simulation_exposure(site)

    datasets = (projections_df, optimizer_df, sim_lineups, exposure_df)
    has_data = any(
        dataset is not None and not (hasattr(dataset, "empty") and dataset.empty)
        for dataset in datasets
    )

    payload: Dict[str, object] = {
        "site": site,
        "metrics": _build_metrics(projections_df, optimizer_df, sim_info, sim_lineups),
        "quickStats": _build_quick_stats(projections_df, optimizer_df, sim_lineups),
        "performanceChart": _performance_chart(sim_lineups),
        "ownershipChart": _ownership_chart(exposure_df),
        "compositionChart": _composition_chart(sim_lineups),
        "radarChart": _radar_chart(sim_lineups, exposure_df),
        "contestTable": _contest_table(site, sim_lineups, sim_info),
        "lineupPreview": _lineup_preview(sim_lineups),
        "playerExposure": _exposure_table(exposure_df),
        "files": {
            "projections": str(projections_info.path) if projections_info else None,
            "optimizer": str(optimizer_info.path) if optimizer_info else None,
            "simulation": str(sim_info.path) if sim_info else None,
            "playerExposure": str(exposure_info.path) if exposure_info else None,
        },
        "lastUpdated": {
            "projections": _format_timestamp(projections_info),
            "optimizer": _format_timestamp(optimizer_info),
            "simulation": _format_timestamp(sim_info),
            "playerExposure": _format_timestamp(exposure_info),
        },
        "hasData": has_data,
    }

    payload["projectionSample"] = []
    if projections_df is not None and not projections_df.empty:
        sample_columns = [col for col in ["name", "team", "pos", "fpts", "own%"] if col in projections_df.columns]
        if sample_columns:
            payload["projectionSample"] = (
                projections_df[sample_columns]
                .sort_values(by=sample_columns[-1], ascending=False)
                .head(12)
                .to_dict(orient="records")
            )

    return payload


app = FastAPI(title="Simulation Dashboard API")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

logger.info(f"Checking for frontend bundle at: {FRONTEND_DIST_DIR}")
logger.info(f"Current working directory: {Path.cwd()}")
logger.info(f"Base directory: {BASE_DIR}")

if FRONTEND_DIST_DIR.exists():
    logger.info(f"✓ Frontend dist directory exists: {FRONTEND_DIST_DIR}")
    
    # List contents of dist directory for debugging
    dist_contents = list(FRONTEND_DIST_DIR.iterdir())
    logger.info(f"Contents of {FRONTEND_DIST_DIR}: {[f.name for f in dist_contents]}")
    
    INDEX_FILE = FRONTEND_DIST_DIR / "index.html"
    if INDEX_FILE.exists():
        logger.info(f"✓ Frontend bundle found at {FRONTEND_DIST_DIR}")
        logger.info(f"✓ index.html size: {INDEX_FILE.stat().st_size} bytes")
        logger.info(f"✓ Mounting React SPA at / with index file: {INDEX_FILE}")
        
        # Mount static assets (CSS, JS, images) with proper caching
        static_assets_dir = FRONTEND_DIST_DIR / "assets"
        if static_assets_dir.exists():
            assets_contents = list(static_assets_dir.iterdir())
            logger.info(f"✓ Assets directory found with {len(assets_contents)} files")
            app.mount(
                "/assets", 
                StaticFiles(directory=static_assets_dir), 
                name="static_assets"
            )
            logger.info(f"✓ Mounted static assets at /assets from {static_assets_dir}")
        else:
            logger.warning(f"⚠ Assets directory not found: {static_assets_dir}")
        
        # Mount the SPA with fallback to index.html for client-side routing
        app.mount(
            "/",
            SPAStaticFiles(directory=FRONTEND_DIST_DIR, html=True),
            name="dashboard",
        )
        logger.info("✓ Successfully mounted React SPA")
    else:
        logger.error(f"✗ Frontend dist directory exists but index.html missing: {INDEX_FILE}")
        logger.error(f"  Available files in dist: {[f.name for f in FRONTEND_DIST_DIR.iterdir()]}")
else:
    logger.error(f"✗ Frontend dist directory not found: {FRONTEND_DIST_DIR}")
    logger.error(f"  Current directory contents: {[f.name for f in BASE_DIR.iterdir() if f.is_dir()]}")
    
    # Check if frontend directory exists at all
    frontend_dir = BASE_DIR / "frontend"
    if frontend_dir.exists():
        logger.info(f"Frontend directory exists: {frontend_dir}")
        frontend_contents = [f.name for f in frontend_dir.iterdir()]
        logger.info(f"Frontend directory contents: {frontend_contents}")
    else:
        logger.error(f"Frontend directory does not exist: {frontend_dir}")
    
    logger.error("  The React dashboard will not be served - only API endpoints available")
    
    @app.get("/", include_in_schema=False)
    async def missing_frontend() -> Dict[str, str]:
        """Provide helpful message when frontend bundle is missing."""
        return {
            "error": "Frontend bundle not available",
            "message": "The React dashboard is not available. Only API endpoints are accessible.",
            "api_docs": "/docs",
            "health_check": "/api/health",
            "available_sites": "/api/dashboard/sites"
        }


@app.get("/api/health", include_in_schema=False)
def healthcheck() -> Dict[str, str]:
    """Provide a friendly landing response for platform health checks."""

    return {
        "status": "ok",
        "message": "MLB DFS Dashboard API is running.",
        "docs": "/docs",
        "sites": "/api/dashboard/sites",
        "dashboard": "/",
    }


@app.get("/favicon.ico", include_in_schema=False)
def favicon() -> Response:
    """Return an empty response so the platform does not log a 404."""

    return Response(status_code=204)


@app.get("/api/dashboard/sites")
def get_sites() -> Dict[str, List[str]]:
    sites = available_sites()
    if not sites:
        # expose the known keys so the UI can still render controls
        sites = sorted(SITE_DIRECTORIES.keys())
    return {"sites": sites}


@app.get("/api/dashboard/site/{site}")
def get_site_dashboard(site: str) -> Dict[str, object]:
    site = site.lower()
    if site not in SITE_DIRECTORIES:
        raise HTTPException(status_code=404, detail="Unknown site.")

    return build_dashboard_payload(site)


@app.get("/api/dashboard")
def get_default_dashboard() -> Dict[str, object]:
    sites = available_sites()
    target_site = sites[0] if sites else next(iter(SITE_DIRECTORIES.keys()))
    data = build_dashboard_payload(target_site)
    data["availableSites"] = sites or list(SITE_DIRECTORIES.keys())
    return data


# Firecrawl API endpoints
@app.post("/api/scrape")
async def scrape_url(request: Dict[str, str]) -> Dict[str, object]:
    """Scrape a single URL using Firecrawl."""
    try:
        url = request.get("url")
        if not url:
            raise HTTPException(status_code=400, detail="URL is required")
        
        client = get_firecrawl_client()
        result = client.scrape_url(url)
        
        return {
            "success": True,
            "data": result,
            "url": url
        }
    except ValueError as e:
        logger.error(f"Configuration error: {str(e)}")
        raise HTTPException(status_code=500, detail="Firecrawl API key not configured")
    except Exception as e:
        logger.error(f"Error scraping URL {url}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to scrape URL: {str(e)}")


@app.post("/api/scrape/mlb-stats")
async def scrape_mlb_stats(request: Dict[str, str]) -> Dict[str, object]:
    """Scrape MLB statistics from a URL with optimized extraction."""
    try:
        url = request.get("url")
        if not url:
            raise HTTPException(status_code=400, detail="URL is required")
        
        client = get_firecrawl_client()
        result = client.scrape_mlb_stats_site(url)
        
        return {
            "success": True,
            "data": result,
            "url": url,
            "type": "mlb_stats"
        }
    except ValueError as e:
        logger.error(f"Configuration error: {str(e)}")
        raise HTTPException(status_code=500, detail="Firecrawl API key not configured")
    except Exception as e:
        logger.error(f"Error scraping MLB stats from {url}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to scrape MLB stats: {str(e)}")


@app.post("/api/scrape/injury-report")
async def scrape_injury_report(request: Dict[str, str]) -> Dict[str, object]:
    """Scrape injury reports with structured extraction."""
    try:
        url = request.get("url")
        if not url:
            raise HTTPException(status_code=400, detail="URL is required")
        
        client = get_firecrawl_client()
        result = client.scrape_injury_report(url)
        
        return {
            "success": True,
            "data": result,
            "url": url,
            "type": "injury_report"
        }
    except ValueError as e:
        logger.error(f"Configuration error: {str(e)}")
        raise HTTPException(status_code=500, detail="Firecrawl API key not configured")
    except Exception as e:
        logger.error(f"Error scraping injury report from {url}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to scrape injury report: {str(e)}")


@app.post("/api/scrape/weather")
async def scrape_weather_data(request: Dict[str, str]) -> Dict[str, object]:
    """Scrape weather data for MLB games."""
    try:
        url = request.get("url")
        if not url:
            raise HTTPException(status_code=400, detail="URL is required")
        
        client = get_firecrawl_client()
        result = client.scrape_weather_data(url)
        
        return {
            "success": True,
            "data": result,
            "url": url,
            "type": "weather_data"
        }
    except ValueError as e:
        logger.error(f"Configuration error: {str(e)}")
        raise HTTPException(status_code=500, detail="Firecrawl API key not configured")
    except Exception as e:
        logger.error(f"Error scraping weather data from {url}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to scrape weather data: {str(e)}")


@app.post("/api/crawl")
async def crawl_website(request: Dict[str, object]) -> Dict[str, object]:
    """Crawl an entire website starting from the given URL."""
    try:
        url = request.get("url")
        if not url:
            raise HTTPException(status_code=400, detail="URL is required")
        
        # Extract crawl parameters
        crawl_params = {k: v for k, v in request.items() if k != "url"}
        
        client = get_firecrawl_client()
        result = client.crawl_website(url, **crawl_params)
        
        return {
            "success": True,
            "data": result,
            "url": url,
            "type": "website_crawl"
        }
    except ValueError as e:
        logger.error(f"Configuration error: {str(e)}")
        raise HTTPException(status_code=500, detail="Firecrawl API key not configured")
    except Exception as e:
        logger.error(f"Error crawling website {url}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to crawl website: {str(e)}")


@app.post("/api/scrape/bulk")
async def bulk_scrape_urls(request: Dict[str, object]) -> Dict[str, object]:
    """Scrape multiple URLs in batch."""
    try:
        urls = request.get("urls")
        if not urls or not isinstance(urls, list):
            raise HTTPException(status_code=400, detail="URLs array is required")
        
        # Extract scrape parameters
        scrape_params = {k: v for k, v in request.items() if k != "urls"}
        
        client = get_firecrawl_client()
        results = client.bulk_scrape_urls(urls, **scrape_params)
        
        return {
            "success": True,
            "data": results,
            "total_urls": len(urls),
            "type": "bulk_scrape"
        }
    except ValueError as e:
        logger.error(f"Configuration error: {str(e)}")
        raise HTTPException(status_code=500, detail="Firecrawl API key not configured")
    except Exception as e:
        logger.error(f"Error bulk scraping URLs: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to bulk scrape URLs: {str(e)}")


# Advanced Extract API endpoints using Firecrawl v2
@app.post("/api/extract")
async def extract_structured_data(request: Dict[str, object]) -> Dict[str, object]:
    """Extract structured data using LLMs from one or multiple URLs."""
    try:
        urls = request.get("urls")
        if not urls or not isinstance(urls, list):
            raise HTTPException(status_code=400, detail="URLs array is required")
        
        prompt = request.get("prompt")
        schema = request.get("schema")
        enable_web_search = request.get("enableWebSearch", False)
        
        if not prompt and not schema:
            raise HTTPException(status_code=400, detail="Either prompt or schema is required")
        
        client = get_firecrawl_client()
        result = client.extract(
            urls=urls,
            prompt=prompt,
            schema=schema,
            enable_web_search=enable_web_search
        )
        
        return {
            "success": True,
            "data": result,
            "urls": urls,
            "type": "extract"
        }
    except ValueError as e:
        logger.error(f"Configuration error: {str(e)}")
        raise HTTPException(status_code=500, detail="Firecrawl API key not configured")
    except Exception as e:
        logger.error(f"Error extracting data: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to extract data: {str(e)}")


@app.post("/api/extract/start")
async def start_extract_job(request: Dict[str, object]) -> Dict[str, object]:
    """Start an extraction job without waiting for completion."""
    try:
        urls = request.get("urls")
        if not urls or not isinstance(urls, list):
            raise HTTPException(status_code=400, detail="URLs array is required")
        
        prompt = request.get("prompt")
        schema = request.get("schema")
        enable_web_search = request.get("enableWebSearch", False)
        
        if not prompt and not schema:
            raise HTTPException(status_code=400, detail="Either prompt or schema is required")
        
        client = get_firecrawl_client()
        result = client.start_extract(
            urls=urls,
            prompt=prompt,
            schema=schema,
            enable_web_search=enable_web_search
        )
        
        return {
            "success": True,
            "job_id": result.id,
            "status": result.status,
            "urls": urls,
            "type": "async_extract"
        }
    except ValueError as e:
        logger.error(f"Configuration error: {str(e)}")
        raise HTTPException(status_code=500, detail="Firecrawl API key not configured")
    except Exception as e:
        logger.error(f"Error starting extract job: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to start extract job: {str(e)}")


@app.get("/api/extract/status/{job_id}")
async def get_extract_job_status(job_id: str) -> Dict[str, object]:
    """Get the status of an extraction job."""
    try:
        client = get_firecrawl_client()
        result = client.get_extract_status(job_id)
        
        return {
            "success": True,
            "job_id": job_id,
            "status": result.status,
            "data": result.data if hasattr(result, 'data') else None,
            "expires_at": result.expires_at if hasattr(result, 'expires_at') else None
        }
    except ValueError as e:
        logger.error(f"Configuration error: {str(e)}")
        raise HTTPException(status_code=500, detail="Firecrawl API key not configured")
    except Exception as e:
        logger.error(f"Error getting job status: {str(e)}")
        raise HTTPException(status_code=404, detail=f"Job not found or expired: {str(e)}")


@app.post("/api/extract/mlb-season-stats")
async def extract_mlb_season_stats(request: Dict[str, object]) -> Dict[str, object]:
    """Extract comprehensive MLB season statistics."""
    try:
        base_url = request.get("url")
        if not base_url:
            raise HTTPException(status_code=400, detail="URL is required")
        
        enable_web_search = request.get("enableWebSearch", True)
        
        client = get_firecrawl_client()
        result = client.extract_mlb_season_stats(base_url, enable_web_search)
        
        return {
            "success": True,
            "data": result,
            "url": base_url,
            "type": "mlb_season_stats"
        }
    except ValueError as e:
        logger.error(f"Configuration error: {str(e)}")
        raise HTTPException(status_code=500, detail="Firecrawl API key not configured")
    except Exception as e:
        logger.error(f"Error extracting MLB season stats: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to extract MLB season stats: {str(e)}")


@app.post("/api/extract/dfs-slate")
async def extract_dfs_slate_info(request: Dict[str, object]) -> Dict[str, object]:
    """Extract DFS slate information."""
    try:
        urls = request.get("urls")
        if not urls or not isinstance(urls, list):
            raise HTTPException(status_code=400, detail="URLs array is required")
        
        client = get_firecrawl_client()
        result = client.extract_dfs_slate_info(urls)
        
        return {
            "success": True,
            "data": result,
            "urls": urls,
            "type": "dfs_slate"
        }
    except ValueError as e:
        logger.error(f"Configuration error: {str(e)}")
        raise HTTPException(status_code=500, detail="Firecrawl API key not configured")
    except Exception as e:
        logger.error(f"Error extracting DFS slate info: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to extract DFS slate info: {str(e)}")


@app.post("/api/extract/betting-lines")
async def extract_betting_lines(request: Dict[str, object]) -> Dict[str, object]:
    """Extract betting lines and odds information."""
    try:
        urls = request.get("urls")
        if not urls or not isinstance(urls, list):
            raise HTTPException(status_code=400, detail="URLs array is required")
        
        client = get_firecrawl_client()
        result = client.extract_betting_lines(urls)
        
        return {
            "success": True,
            "data": result,
            "urls": urls,
            "type": "betting_lines"
        }
    except ValueError as e:
        logger.error(f"Configuration error: {str(e)}")
        raise HTTPException(status_code=500, detail="Firecrawl API key not configured")
    except Exception as e:
        logger.error(f"Error extracting betting lines: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to extract betting lines: {str(e)}")


@app.post("/api/extract/advanced-injury-report")
async def extract_advanced_injury_report(request: Dict[str, object]) -> Dict[str, object]:
    """Extract comprehensive injury reports with timelines."""
    try:
        urls = request.get("urls")
        if not urls or not isinstance(urls, list):
            raise HTTPException(status_code=400, detail="URLs array is required")
        
        client = get_firecrawl_client()
        result = client.extract_advanced_injury_report(urls)
        
        return {
            "success": True,
            "data": result,
            "urls": urls,
            "type": "advanced_injury_report"
        }
    except ValueError as e:
        logger.error(f"Configuration error: {str(e)}")
        raise HTTPException(status_code=500, detail="Firecrawl API key not configured")
    except Exception as e:
        logger.error(f"Error extracting injury report: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to extract injury report: {str(e)}")


# Schema and template endpoints
@app.get("/api/extract/schemas")
async def get_extraction_schemas() -> Dict[str, object]:
    """Get available extraction schemas."""
    return {
        "schemas": {
            "player_stats": MLBExtractionSchemas.player_stats_schema(),
            "team_standings": MLBExtractionSchemas.team_standings_schema(),
            "injury_report": MLBExtractionSchemas.injury_report_schema(),
            "weather_conditions": MLBExtractionSchemas.weather_conditions_schema(),
            "dfs_slate": MLBExtractionSchemas.dfs_slate_schema(),
            "betting_lines": MLBExtractionSchemas.betting_lines_schema(),
            "roster_moves": MLBExtractionSchemas.roster_moves_schema()
        }
    }


@app.get("/api/extract/prompts")
async def get_extraction_prompts() -> Dict[str, object]:
    """Get available extraction prompts."""
    return {
        "prompts": {
            "player_stats": MLBExtractionPrompts.player_stats_prompt(),
            "team_standings": MLBExtractionPrompts.team_standings_prompt(),
            "injury_report": MLBExtractionPrompts.injury_report_prompt(),
            "weather_conditions": MLBExtractionPrompts.weather_conditions_prompt(),
            "dfs_slate": MLBExtractionPrompts.dfs_slate_prompt(),
            "betting_lines": MLBExtractionPrompts.betting_lines_prompt(),
            "roster_moves": MLBExtractionPrompts.roster_moves_prompt()
        }
    }


@app.post("/api/extract/template")
async def extract_with_template(request: Dict[str, object]) -> Dict[str, object]:
    """Extract data using a pre-built template."""
    try:
        template_type = request.get("template")
        urls = request.get("urls")
        
        if not template_type:
            raise HTTPException(status_code=400, detail="Template type is required")
        if not urls or not isinstance(urls, list):
            raise HTTPException(status_code=400, detail="URLs array is required")
        
        # Get schema and prompt for template
        schema_methods = {
            "player_stats": MLBExtractionSchemas.player_stats_schema,
            "team_standings": MLBExtractionSchemas.team_standings_schema,
            "injury_report": MLBExtractionSchemas.injury_report_schema,
            "weather_conditions": MLBExtractionSchemas.weather_conditions_schema,
            "dfs_slate": MLBExtractionSchemas.dfs_slate_schema,
            "betting_lines": MLBExtractionSchemas.betting_lines_schema,
            "roster_moves": MLBExtractionSchemas.roster_moves_schema
        }
        
        prompt_methods = {
            "player_stats": MLBExtractionPrompts.player_stats_prompt,
            "team_standings": MLBExtractionPrompts.team_standings_prompt,
            "injury_report": MLBExtractionPrompts.injury_report_prompt,
            "weather_conditions": MLBExtractionPrompts.weather_conditions_prompt,
            "dfs_slate": MLBExtractionPrompts.dfs_slate_prompt,
            "betting_lines": MLBExtractionPrompts.betting_lines_prompt,
            "roster_moves": MLBExtractionPrompts.roster_moves_prompt
        }
        
        if template_type not in schema_methods:
            raise HTTPException(status_code=400, detail=f"Unknown template type: {template_type}")
        
        schema = schema_methods[template_type]()
        prompt = prompt_methods[template_type]()
        enable_web_search = request.get("enableWebSearch", False)
        
        client = get_firecrawl_client()
        result = client.extract(
            urls=urls,
            prompt=prompt,
            schema=schema,
            enable_web_search=enable_web_search
        )
        
        return {
            "success": True,
            "data": result,
            "template": template_type,
            "urls": urls,
            "type": "template_extract"
        }
    except ValueError as e:
        logger.error(f"Configuration error: {str(e)}")
        raise HTTPException(status_code=500, detail="Firecrawl API key not configured")
    except Exception as e:
        logger.error(f"Error extracting with template: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to extract with template: {str(e)}")


# MLB Tools API endpoints
@app.post("/api/run/optimizer")
async def run_optimizer(request: Dict[str, object]) -> Dict[str, object]:
    """Run the MLB optimizer with specified parameters."""
    try:
        site = request.get("site", "dk")
        num_lineups = request.get("num_lineups", 20)
        num_uniques = request.get("num_uniques", 1)
        
        # Import and run optimizer
        from mlb_optimizer import MLB_Optimizer
        
        opto = MLB_Optimizer(site, num_lineups, num_uniques)
        opto.optimize()
        opto.output()
        
        return {
            "success": True,
            "message": f"Optimizer completed for {site} - generated {num_lineups} lineups",
            "site": site,
            "num_lineups": num_lineups,
            "num_uniques": num_uniques
        }
    except Exception as e:
        logger.error(f"Error running optimizer: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to run optimizer: {str(e)}")


@app.post("/api/run/simulation")
async def run_simulation(request: Dict[str, object]) -> Dict[str, object]:
    """Run the MLB GPP simulator with specified parameters."""
    try:
        site = request.get("site", "dk")
        field_size = request.get("field_size", 20)
        num_iterations = request.get("num_iterations", 500)
        use_contest_data = request.get("use_contest_data", False)
        use_file_upload = request.get("use_file_upload", False)
        
        # Import and run simulator
        from mlb_gpp_simulator import MLB_GPP_Simulator
        
        sim = MLB_GPP_Simulator(
            site,
            field_size,
            num_iterations,
            use_contest_data,
            use_file_upload,
            True  # match_lineup_input_to_field_size
        )
        sim.simulate()
        sim.output()
        
        return {
            "success": True,
            "message": f"Simulation completed for {site} - {num_iterations} iterations with field size {field_size}",
            "site": site,
            "field_size": field_size,
            "num_iterations": num_iterations
        }
    except Exception as e:
        logger.error(f"Error running simulation: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to run simulation: {str(e)}")


@app.post("/api/upload/projections")
async def upload_projections(request: Dict[str, object]) -> Dict[str, object]:
    """Process uploaded projection file."""
    try:
        site = request.get("site", "dk")
        file_data = request.get("file_data")
        filename = request.get("filename", "projections.csv")
        
        if not file_data:
            raise HTTPException(status_code=400, detail="No file data provided")
        
        # Determine site directory
        site_dir = SITE_DIRECTORIES.get(site, BASE_DIR / f"{site}_data")
        if not site_dir.exists():
            site_dir.mkdir(parents=True, exist_ok=True)
        
        # Save uploaded file
        projection_path = site_dir / "projections.csv"
        
        # In a real implementation, you'd decode base64 file_data and write it
        # For now, we'll just acknowledge the upload
        
        return {
            "success": True,
            "message": f"Projections uploaded successfully for {site}",
            "site": site,
            "filename": filename,
            "saved_to": str(projection_path)
        }
    except Exception as e:
        logger.error(f"Error uploading projections: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to upload projections: {str(e)}")


if __name__ == "__main__":  # pragma: no cover - manual execution helper
    import uvicorn

    uvicorn.run("src.dashboard_api:app", host="0.0.0.0", port=8000, reload=True)

